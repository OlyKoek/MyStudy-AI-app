{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/OlyKoek/MyStudy-AI-app/blob/colab/create_multimodal_embeding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SLj01Un8xV5g"
   },
   "source": [
    "# マルチモーダルなエンベディングモデルの作成\n",
    "# Mini-CLIP: Text + Image shared embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Text: multilingual MiniLM (TinyBERT)\n",
    "- Image: MobileNetV3-Small\n",
    "- Projection Head: 256 dimention shared space\n",
    "- Loss: CLIP-style contrastive loss\n",
    "- VectorStore: SimpleVetorDB    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio --quiet\n",
    "!pip install transformers umap-learn matplotlib pandas scikit-learn plotly --quiet\n",
    "\n",
    "import os\n",
    "import json\n",
    "from io import BytesIO\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "from PIL import Image\n",
    "import requests\n",
    "from IPython.display import display\n",
    "\n",
    "from google.colab.output import download\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "\n",
    "# モデルと次元数\n",
    "TEXT_MODEL_NAME = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "EMBED_DIM = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    ベクトル a, b のコサイン類似度を返すための関数\n",
    "    \"\"\"\n",
    "    a_flat = a.flatten()\n",
    "    b_flat = b.flatten()\n",
    "    denom = (np.linalg.norm(a_flat) * np.linalg.norm(b_flat) + 1e-8)\n",
    "    return float(np.dot(a_flat, b_flat) / denom)\n",
    "\n",
    "\n",
    "def load_image(x):\n",
    "    \"\"\"\n",
    "    Imageを読み込むための関数\n",
    "    入力 x:\n",
    "      - str(URL or file path)\n",
    "      - PIL.Image.Image\n",
    "    \"\"\"\n",
    "    if isinstance(x, Image.Image):\n",
    "        return x.convert(\"RGB\")\n",
    "\n",
    "    if isinstance(x, str):\n",
    "        if x.startswith(\"https://\") or x.startswith(\"http://\"):\n",
    "            response = requests.get(x)\n",
    "            img = Image.open(BytesIO(response.content))\n",
    "            return img.convert(\"RGB\")\n",
    "        else:\n",
    "            img = Image.open(x)\n",
    "            return img.convert(\"RGB\")\n",
    "\n",
    "    raise ValueError(f\"Unsupported image input type: {type(x)}\")\n",
    "\n",
    "\n",
    "def plot_umap_matplotlib(embs, labels, texts=None, title=\"UMAP\"):\n",
    "    \"\"\"\n",
    "    MatplotlibでUMAPを描画\n",
    "    embs: (N, D) numpy array\n",
    "    labels: list[str] same length as N\n",
    "    texts:  list[str] hover表示用（任意）\n",
    "    \"\"\"\n",
    "    reducer = umap.UMAP(n_neighbors=10, min_dist=0.1, metric=\"cosine\")\n",
    "    coords = reducer.fit_transform(embs)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for i, lab in enumerate(labels):\n",
    "        plt.scatter(coords[i, 0], coords[i, 1])\n",
    "        if texts is not None:\n",
    "            plt.text(coords[i, 0], coords[i, 1], texts[i][:10], fontsize=8)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_umap_plotly(embs, labels, texts=None, extra_meta=None, title=\"UMAP (Interactive)\"):\n",
    "    \"\"\"\n",
    "    PlotlyでインタラクティブなUMAPを描画\n",
    "    \"\"\"\n",
    "    reducer = umap.UMAP(n_neighbors=10, min_dist=0.1, metric=\"cosine\")\n",
    "    coords = reducer.fit_transform(embs)\n",
    "\n",
    "    data = {\n",
    "        \"x\": coords[:, 0],\n",
    "        \"y\": coords[:, 1],\n",
    "        \"type\": labels,\n",
    "    }\n",
    "    if texts is not None:\n",
    "        data[\"text\"] = texts\n",
    "    if extra_meta is not None:\n",
    "        for k, v in extra_meta.items():\n",
    "            data[k] = v\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    fig = px.scatter(\n",
    "        df,\n",
    "        x=\"x\",\n",
    "        y=\"y\",\n",
    "        color=\"type\",\n",
    "        hover_data=list(data.keys()),\n",
    "        title=title,\n",
    "        width=800,\n",
    "        height=600,\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Classes\n",
    "- TextEncoder\n",
    "- ImageEncoder\n",
    "- MiniCLIP\n",
    "- SimpleVectorDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    TinyBERT系でテキストをエンコードするクラス\n",
    "    Linear Projectionで256次元に射影変換\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name: str, out_dim: int = EMBED_DIM, device: torch.device = DEVICE):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name).to(device)\n",
    "        hidden = self.model.config.hidden_size\n",
    "        self.projector  = nn.Linear(hidden, out_dim).to(device)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        HuggingFaceスタイルのForward関数を参考に実装\n",
    "        B: バッチサイズ\n",
    "        3: チャネル数(RGB)\n",
    "        H, W: 画像の高さと幅\n",
    "        T: トークン数\n",
    "        H: モデルの隠れ層次元数\n",
    "        out_dim: 射影後の次元数\n",
    "        \"\"\" \n",
    "        outputs = self.model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1)  # (B, T, H) -> (B, H)\n",
    "        projected = self.projector(embeddings)  # (B, out_dim) \n",
    "        return projected\n",
    "\n",
    "    def encode(self, texts, normalize: bool = True) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        textsをエンコードしてベクトルを返す関数\n",
    "\n",
    "        texts: list[str]\n",
    "        normalize: bool - 出力ベクトルを正規化するかどうか\n",
    "        \"\"\"\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "\n",
    "        inputs = self.tokenizer(\n",
    "            texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            emb = outputs.last_hidden_state.mean(dim=1)  # (B, H)\n",
    "\n",
    "        proj = self.projector(emb)  # (B, out_dim)\n",
    "        \n",
    "        if normalize:\n",
    "            proj = proj / (proj.norm(dim=1, keepdim=True) + 1e-8)\n",
    "\n",
    "        return proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    MobileNetV3-Smallで画像をエンコードするクラス\n",
    "    Linear Projectionで256次元に射影変換\n",
    "    \"\"\"\n",
    "    def __init__(self, out_dim: int = EMBED_DIM, device: torch.device = DEVICE):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.model = models.mobilenet_v3_small(pretrained=True)\n",
    "        self.model.eval()\n",
    "        self.features = self.model.features.to(device)\n",
    "        self.projector = nn.Linear(576, out_dim).to(device)  # MobileNetV3-Smallの最終特徴量次元数は576\n",
    "\n",
    "        self.transform = T.Compose([\n",
    "            T.Resize((224, 224)),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize([0.485, 0.456, 0.406],\n",
    "                        [0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "    def encode(self, images, normalize: bool = True) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        画像リストをエンコードしてベクトルを返す関数\n",
    "\n",
    "        images: list[PIL.Image.Image] または 単一のPIL.Image.Image\n",
    "        normalize: bool - 出力ベクトルを正規化するかどうか\n",
    "        \"\"\"\n",
    "        single = False\n",
    "        if not isinstance(images, (list, tuple)):\n",
    "            images = [images]\n",
    "            single = True\n",
    "\n",
    "        tensors = []\n",
    "        for img in images:\n",
    "            pil_img = load_image(img)\n",
    "            tensor = self.transform(pil_img) # (3, H, W)\n",
    "            tensors.append(tensor)\n",
    "        batch = torch.stack(tensors, dim=0).to(self.device)  # (B, 3, H, W)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            feat = self.features(batch)  # (B, 576, 7, 7)\n",
    "            feat = feat.mean(dim=[2, 3])  # (B, 576)\n",
    "        \n",
    "        proj = self.projector(feat)  # (B, out_dim)\n",
    "        \n",
    "        if normalize:\n",
    "            proj = proj / (proj.norm(dim=1, keepdim=True) + 1e-8)\n",
    "\n",
    "        if single:\n",
    "            return proj # (1, out_dim)\n",
    "        return proj  # (B, out_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniCLIP:\n",
    "    \"\"\"\n",
    "    TextEncoder と ImageEncoder を共有 256次元空間に揃え、\n",
    "    CLIP風の対照学習で projector 層のみを学習するクラス。\n",
    "    \"\"\"\n",
    "    def __init__(self, text_encoder: TextEncoder, image_encoder: ImageEncoder, temperature: float = 0.07):\n",
    "        self.text_encoder = text_encoder\n",
    "        self.image_encoder = image_encoder\n",
    "        self.temperature = temperature\n",
    "\n",
    "        params = list(self.text_encoder.projector.parameters()) + \\\n",
    "                 list(self.image_encoder.projector.parameters())\n",
    "        self.optimizer = AdamW(params, lr=1e-4)\n",
    "    \n",
    "\n",
    "    def compute_loss(self, img_vecs: torch.Tensor, txt_vecs: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        CLIP風の対照学習の損失を計算する関数\n",
    "        img_vecs: (B, D)\n",
    "        txt_vecs: (B, D)\n",
    "        \"\"\"\n",
    "        # 類似度行列[B, B]を計算\n",
    "        sim_matrix = torch.matmul(img_vecs, txt_vecs.T)\n",
    "        sim_matrix = sim_matrix / self.temperature\n",
    "\n",
    "        labels = torch.arange(len(img_vecs), device=sim_matrix.device)\n",
    "        \n",
    "        loss_img2txt = F.cross_entropy(sim_matrix, labels)\n",
    "        loss_txt2img = F.cross_entropy(sim_matrix.T, labels)\n",
    "\n",
    "        loss = (loss_img2txt + loss_txt2img) / 2.0\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def train(self, pairs, epochs: int = 10, batch_size: int = 32):\n",
    "        \"\"\"\n",
    "        学習を実行する関数\n",
    "        pairs: list of (PIL.Image.Image, str)\n",
    "        \"\"\"\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            texts  = [p[\"text\"]  for p in pairs]\n",
    "            images = [p[\"image\"] for p in pairs]\n",
    "\n",
    "            txt_vecs = self.text_encoder.encode(texts, normalize=True)  # (B, D)\n",
    "            img_vecs = self.image_encoder.encode(images, normalize=True)  # (B, D)\n",
    "\n",
    "            loss = self.compute_loss(img_vecs, txt_vecs)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "        print(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleVectorDB:\n",
    "    \"\"\"\n",
    "    非常にシンプルなベクトルストア。\n",
    "    - items: List[{\"vec\": np.ndarray, \"type\": ..., \"text\": ..., \"image\": ...}]\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.items = []\n",
    "\n",
    "    def add(self, vec: np.ndarray, metadata: dict):\n",
    "        self.items.append({\n",
    "            \"vec\": vec.astype(\"float32\"),\n",
    "            **metadata\n",
    "        })\n",
    "\n",
    "    def build_from_pairs(self, text_encoder: TextEncoder, image_encoder: ImageEncoder, pairs):\n",
    "        \"\"\"\n",
    "        pairs: List[{\"image\": ..., \"text\": ...}]\n",
    "        \"\"\"\n",
    "        self.items = []\n",
    "        for p in pairs:\n",
    "            # text embedding\n",
    "            t_vec = text_encoder.encode(p[\"text\"]).cpu().detach().numpy()[0]\n",
    "            self.add(t_vec, {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": p[\"text\"],\n",
    "                \"image\": p[\"image\"],\n",
    "            })\n",
    "\n",
    "            # image embedding\n",
    "            i_vec = image_encoder.encode(p[\"image\"]).cpu().detach().numpy()[0]\n",
    "            self.add(i_vec, {\n",
    "                \"type\": \"image\",\n",
    "                \"text\": p[\"text\"],\n",
    "                \"image\": p[\"image\"],\n",
    "            })\n",
    "\n",
    "    def search(self, query_vec: np.ndarray, top_k: int = 5, type_filter: str = None):\n",
    "        \"\"\"\n",
    "        query_vec: np.ndarray [D]\n",
    "        type_filter: \"text\" or \"image\" or None\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        for item in self.items:\n",
    "            if type_filter is not None and item[\"type\"] != type_filter:\n",
    "                continue\n",
    "            sim = cosine_sim(query_vec, item[\"vec\"])\n",
    "            results.append((sim, item))\n",
    "\n",
    "        results.sort(key=lambda x: x[0], reverse=True)\n",
    "        return results[:top_k]\n",
    "\n",
    "    def to_jsonable(self):\n",
    "        \"\"\"\n",
    "        JSON保存用に numpy 配列を list に変換\n",
    "        \"\"\"\n",
    "        json_items = []\n",
    "        for item in self.items:\n",
    "            j = dict(item)\n",
    "            j[\"vec\"] = item[\"vec\"].tolist()\n",
    "            json_items.append(j)\n",
    "        return json_items\n",
    "\n",
    "    @staticmethod\n",
    "    def from_json(data):\n",
    "        db = SimpleVectorDB()\n",
    "        for item in data:\n",
    "            vec = np.array(item[\"vec\"], dtype=\"float32\")\n",
    "            meta = {k: v for k, v in item.items() if k != \"vec\"}\n",
    "            db.add(vec, meta)\n",
    "        return db\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DogDemoDatasetLoader:\n",
    "    \"\"\"\n",
    "    とりあえず dog.ceo のデモデータを返す Loader。\n",
    "    将来、ArtBenchLoader / MemotionLoader / CustomLoader を同じIFで追加できる。\n",
    "    \"\"\"\n",
    "    def load(self):\n",
    "        pairs = [\n",
    "            {\n",
    "                \"image\": \"https://images.dog.ceo/breeds/chow/n02112137_16777.jpg\",\n",
    "                \"text\": \"白いフワフワの犬\"\n",
    "            },\n",
    "            {\n",
    "                \"image\": \"https://images.dog.ceo/breeds/pitbull/20190710_143021.jpg\",\n",
    "                \"text\": \"黒と白の犬が緑の草原の中にいる\"\n",
    "            },\n",
    "            {\n",
    "                \"image\": \"https://images.dog.ceo/breeds/poodle-toy/n02113624_9550.jpg\",\n",
    "                \"text\": \"ふわふわの子犬の写真\"\n",
    "            },\n",
    "            {\n",
    "                \"image\": \"https://images.dog.ceo/breeds/eskimo/n02109961_8185.jpg\",\n",
    "                \"text\": \"白い犬が石畳の上に座っている\"\n",
    "            }\n",
    "        ]\n",
    "        return pairs\n",
    "        \n",
    "loader = DogDemoDatasetLoader()\n",
    "image_text_pairs = loader.load()\n",
    "len(image_text_pairs), image_text_pairs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MiniClip作って学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder = TextEncoder(TEXT_MODEL_NAME, out_dim=EMBED_DIM, device=DEVICE)\n",
    "image_encoder = ImageEncoder(out_dim=EMBED_DIM, device=DEVICE)\n",
    "\n",
    "mini_clip = MiniCLIP(text_encoder, image_encoder, temperature=0.07)\n",
    "\n",
    "mini_clip.train(image_text_pairs, epochs=10, batch_size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VectorDB構築＆検索＆可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VectorDB構築\n",
    "vecdb = SimpleVectorDB()\n",
    "vecdb.build_from_pairs(text_encoder, image_encoder, image_text_pairs)\n",
    "print(f\"VectorDB size: {len(vecdb.items)} items\")\n",
    "\n",
    "# テキストでクエリ\n",
    "query_text = \"白い犬\"\n",
    "query_vec = text_encoder.encode(query_text).cpu().detach().numpy()[0]\n",
    "\n",
    "results = vecdb.search(query_vec, top_k=5)\n",
    "for sim, item in results:\n",
    "    print(f\"sim={sim:.3f} | type={item['type']} | text={item['text']} | image={item['image']}\")\n",
    "    img = load_image(item[\"image\"])\n",
    "    img_small = img.resize((150, 100))\n",
    "    display(img_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UMAP 可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs = np.stack([it[\"vec\"] for it in vecdb.items], axis=0)\n",
    "labels = [it[\"type\"] for it in vecdb.items]\n",
    "texts = [it[\"text\"] for it in vecdb.items]\n",
    "\n",
    "# plot_umap_matplotlib(embs, labels, texts, title=\"Mini-CLIP: Dog Demo (Matplotlib)\")\n",
    "plot_umap_plotly(embs, labels, texts, title=\"Mini-CLIP: Dog Demo (Plotly)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"artifacts\", exist_ok=True)\n",
    "\n",
    "# projector 保存\n",
    "torch.save(text_encoder.projector.state_dict(), \"artifacts/text_projector.pt\")\n",
    "torch.save(image_encoder.projector.state_dict(), \"artifacts/image_projector.pt\")\n",
    "\n",
    "# VectorDB 保存\n",
    "with open(\"artifacts/vector_db.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(vecdb.to_jsonable(), f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Saved to ./artifacts/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AARYzRXm1AT8"
   },
   "source": [
    "#### 日本語表示対応"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 日本語フォントをダウンロードする。\n",
    "# !apt-get -y install fonts-ipafont-gothic\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.rcParams['font.family'] = 'IPAGothic'\n",
    "\n",
    "# import shutil\n",
    "# import os\n",
    "# # フォントキャッシュを削除\n",
    "# font_cache_path = os.path.expanduser(\"~/.cache/matplotlib\")\n",
    "# if os.path.exists(font_cache_path):\n",
    "#     shutil.rmtree(font_cache_path)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM2V5BSBDpravpSMPcLIBN5",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
