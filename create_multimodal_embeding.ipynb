{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OlyKoek/MyStudy-AI-app/blob/feature-update-ml/create_multimodal_embeding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLj01Un8xV5g"
      },
      "source": [
        "# ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãªã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ\n",
        "# Mini-CLIP: Text + Image shared embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUXMBWYCsVfa"
      },
      "source": [
        "- Text: multilingual MiniLM (TinyBERT)\n",
        "- Image: MobileNetV3-Small\n",
        "- Projection Head: 256 dimention shared space\n",
        "- Loss: CLIP-style contrastive loss\n",
        "- VectorStore: SimpleVetorDB    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L44V18kXsVfa"
      },
      "source": [
        "# Import and Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KdIYRG3ysVfa",
        "outputId": "cc057256-fbaa-4006-a140-ed3d6c8950b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.12/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.12/dist-packages (from kaggle) (6.3.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2025.11.12)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (from kaggle) (3.4.4)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from kaggle) (3.11)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.12/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.32.4)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.12/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.12/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.5.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from kaggle) (0.5.1)\n",
            "DEVICE: cpu\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision torchaudio --quiet\n",
        "!pip install transformers umap-learn matplotlib pandas scikit-learn plotly --quiet\n",
        "\n",
        "!pip install kaggle\n",
        "\n",
        "import os\n",
        "import json\n",
        "from io import BytesIO\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import AdamW\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as T\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import umap\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "\n",
        "from PIL import Image\n",
        "import requests\n",
        "from IPython.display import display\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"DEVICE:\", DEVICE)\n",
        "\n",
        "# ãƒ¢ãƒ‡ãƒ«ã¨æ¬¡å…ƒæ•°\n",
        "TEXT_MODEL_NAME = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
        "EMBED_DIM = 256"
      ]
    },
    {
      "metadata": {
        "id": "0HtGf0HEXEa5",
        "outputId": "19663b93-9849-445c-a42a-8176d4498564",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"KAGGLE_USERNAME\"] = userdata.get('username')\n",
        "os.environ[\"KAGGLE_KEY\"] = userdata.get('token')\n",
        "\n",
        "!kaggle datasets list"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ref                                                             title                                                     size  lastUpdated                 downloadCount  voteCount  usabilityRating  \n",
            "--------------------------------------------------------------  --------------------------------------------------  ----------  --------------------------  -------------  ---------  ---------------  \n",
            "wardabilal/spotify-global-music-dataset-20092025                Spotify Global Music Dataset (2009â€“2025)               1289021  2025-11-11 09:43:05.933000           3288         58  1.0              \n",
            "sadiajavedd/students-academic-performance-dataset               Students_Academic_Performance_Dataset                     8907  2025-10-23 04:16:35.563000           9208        230  1.0              \n",
            "zahranusrat/salary                                              Salary Insights by Job Role (2024)                      110295  2025-11-11 07:02:27.350000           1309         24  1.0              \n",
            "ayeshaimran123/social-media-and-mental-health-balance           Social Media and Mental Health Balance                    5941  2025-10-26 07:51:53.380000           8214        114  1.0              \n",
            "shahzadi786/world-smartphone-market-2025                        World Smartphone Market 2025                             17795  2025-11-09 04:52:42.650000           3704         87  1.0              \n",
            "zubairamuti/bmw-car-sales-record-2010-2024                      BMW Car Sales Record (2010-2024)                        853356  2025-11-19 04:50:35.480000           1073         42  1.0              \n",
            "zubairdhuddi/shopping-dataset                                   Shopping Behavior Dataset                                72157  2025-11-19 09:39:46.533000           1088         24  1.0              \n",
            "khushikyad001/ai-impact-on-jobs-2030                            AI Impact on Jobs 2030                                   87410  2025-11-09 17:58:05.410000           2405         56  1.0              \n",
            "wardabilal/student-stress-analysis                              Student Stress Analysis                                   1729  2025-11-01 09:14:39.367000           3021         63  1.0              \n",
            "alizabrand/school-performance-analysis                          School Performance Analysis                               7847  2025-11-12 18:11:11.897000            991         29  1.0              \n",
            "kainatjamil12/niteee                                            Netflix Movies and TV Shows Comprehensive Catalogs     1401948  2025-11-19 04:22:09.357000           1112         25  1.0              \n",
            "ayeshasiddiqa123/cars-pre                                       Car Price Analysis Dataset                               46557  2025-11-06 16:38:07.487000           2997         72  1.0              \n",
            "parthpatel2130/realistic-loan-approval-dataset-us-and-canada    Realistic Loan Approval Dataset | US & Canada          1717268  2025-11-01 04:33:16.737000           1004         29  1.0              \n",
            "praveensoni06/1500-latest-movies-datasets-2025                  10000 latest movies datasets 2025                      1397000  2025-11-12 08:35:33.990000            823         21  1.0              \n",
            "kainatjamil12/housing                                           ğŸ¡ Housing Price Dataset â€” Factors Affecting Home          4740  2025-11-08 11:00:08.757000           1748         33  1.0              \n",
            "saadaliyaseen/shopping-behaviour-dataset                        Shopping Behaviour Dataset                               72165  2025-11-16 07:46:33.303000            826         28  1.0              \n",
            "umuttuygurr/e-commerce-customer-behavior-and-sales-analysis-tr  E-Commerce Customer Behavior & Sales Analysis -TR       584451  2025-11-09 07:40:27.120000           5269        102  1.0              \n",
            "shaistashahid/data-science-jobs                                 Data Science Jobs                                         2012  2025-11-19 19:33:52.880000            350         24  1.0              \n",
            "ayeshasiddiqa123/customer-shopping-behavior-dataset             Customer Shopping Behavior Dataset                       72157  2025-11-09 11:12:18.700000           1457         28  1.0              \n",
            "ayeshaseherr/student-performance                                Student Performance Factors Dataset                      96178  2025-11-12 05:50:48.240000           1673         38  1.0              \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FKq9_FRsVfb"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Dhd5nuW4sVfb"
      },
      "outputs": [],
      "source": [
        "def cosine_sim(a: np.ndarray, b: np.ndarray) -> float:\n",
        "    \"\"\"\n",
        "    ãƒ™ã‚¯ãƒˆãƒ« a, b ã®ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã‚’è¿”ã™ãŸã‚ã®é–¢æ•°\n",
        "    \"\"\"\n",
        "    a_flat = a.flatten()\n",
        "    b_flat = b.flatten()\n",
        "    denom = (np.linalg.norm(a_flat) * np.linalg.norm(b_flat) + 1e-8)\n",
        "    return float(np.dot(a_flat, b_flat) / denom)\n",
        "\n",
        "\n",
        "def load_image(x):\n",
        "    \"\"\"\n",
        "    Imageã‚’èª­ã¿è¾¼ã‚€ãŸã‚ã®é–¢æ•°\n",
        "    å…¥åŠ› x:\n",
        "      - str(URL or file path)\n",
        "      - PIL.Image.Image\n",
        "    \"\"\"\n",
        "    if isinstance(x, Image.Image):\n",
        "        return x.convert(\"RGB\")\n",
        "\n",
        "    if isinstance(x, str):\n",
        "        if x.startswith(\"https://\") or x.startswith(\"http://\"):\n",
        "            response = requests.get(x)\n",
        "            img = Image.open(BytesIO(response.content))\n",
        "            return img.convert(\"RGB\")\n",
        "        else:\n",
        "            img = Image.open(x)\n",
        "            return img.convert(\"RGB\")\n",
        "\n",
        "    raise ValueError(f\"Unsupported image input type: {type(x)}\")\n",
        "\n",
        "\n",
        "def plot_umap_matplotlib(embs, labels, texts=None, title=\"UMAP\"):\n",
        "    \"\"\"\n",
        "    Matplotlibã§UMAPã‚’æç”»\n",
        "    embs: (N, D) numpy array\n",
        "    labels: list[str] same length as N\n",
        "    texts:  list[str] hoverè¡¨ç¤ºç”¨ï¼ˆä»»æ„ï¼‰\n",
        "    \"\"\"\n",
        "    reducer = umap.UMAP(n_neighbors=10, min_dist=0.1, metric=\"cosine\")\n",
        "    coords = reducer.fit_transform(embs)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    for i, lab in enumerate(labels):\n",
        "        plt.scatter(coords[i, 0], coords[i, 1])\n",
        "        if texts is not None:\n",
        "            plt.text(coords[i, 0], coords[i, 1], texts[i][:10], fontsize=8)\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_umap_plotly(embs, labels, texts=None, extra_meta=None, title=\"UMAP (Interactive)\"):\n",
        "    \"\"\"\n",
        "    Plotlyã§ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªUMAPã‚’æç”»\n",
        "    \"\"\"\n",
        "    reducer = umap.UMAP(n_neighbors=10, min_dist=0.1, metric=\"cosine\")\n",
        "    coords = reducer.fit_transform(embs)\n",
        "\n",
        "    data = {\n",
        "        \"x\": coords[:, 0],\n",
        "        \"y\": coords[:, 1],\n",
        "        \"type\": labels,\n",
        "    }\n",
        "    if texts is not None:\n",
        "        data[\"text\"] = texts\n",
        "    if extra_meta is not None:\n",
        "        for k, v in extra_meta.items():\n",
        "            data[k] = v\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "    fig = px.scatter(\n",
        "        df,\n",
        "        x=\"x\",\n",
        "        y=\"y\",\n",
        "        color=\"type\",\n",
        "        hover_data=list(data.keys()),\n",
        "        title=title,\n",
        "        width=800,\n",
        "        height=600,\n",
        "    )\n",
        "    fig.show()\n",
        "\n",
        "\n",
        "def unpickle(file):\n",
        "    \"\"\"\n",
        "    CIFARãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®pickleã‚’èª­ã¿è¾¼ã‚€\n",
        "    \"\"\"\n",
        "    with open(file, 'rb') as fo:\n",
        "        d = pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1Q39TFCsVfb"
      },
      "source": [
        "## Core Classes\n",
        "- TextEncoder\n",
        "- ImageEncoder\n",
        "- MiniCLIP\n",
        "- SimpleVectorDB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "yJ5AezfqsVfc"
      },
      "outputs": [],
      "source": [
        "class TextEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    TinyBERTç³»ã§ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã™ã‚‹ã‚¯ãƒ©ã‚¹\n",
        "    Linear Projectionã§256æ¬¡å…ƒã«å°„å½±å¤‰æ›\n",
        "    \"\"\"\n",
        "    def __init__(self, model_name: str, out_dim: int = EMBED_DIM, device: torch.device = DEVICE):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModel.from_pretrained(model_name).to(device)\n",
        "        hidden = self.model.config.hidden_size\n",
        "        self.projector  = nn.Linear(hidden, out_dim).to(device)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"\n",
        "        HuggingFaceã‚¹ã‚¿ã‚¤ãƒ«ã®Forwardé–¢æ•°ã‚’å‚è€ƒã«å®Ÿè£…\n",
        "        B: ãƒãƒƒãƒã‚µã‚¤ã‚º\n",
        "        3: ãƒãƒ£ãƒãƒ«æ•°(RGB)\n",
        "        H, W: ç”»åƒã®é«˜ã•ã¨å¹…\n",
        "        T: ãƒˆãƒ¼ã‚¯ãƒ³æ•°\n",
        "        H: ãƒ¢ãƒ‡ãƒ«ã®éš ã‚Œå±¤æ¬¡å…ƒæ•°\n",
        "        out_dim: å°„å½±å¾Œã®æ¬¡å…ƒæ•°\n",
        "        \"\"\"\n",
        "        outputs = self.model(**inputs)\n",
        "        embeddings = outputs.last_hidden_state.mean(dim=1)  # (B, T, H) -> (B, H)\n",
        "        projected = self.projector(embeddings)  # (B, out_dim)\n",
        "        return projected\n",
        "\n",
        "    def encode(self, texts, normalize: bool = True) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        textsã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ã¦ãƒ™ã‚¯ãƒˆãƒ«ã‚’è¿”ã™é–¢æ•°\n",
        "\n",
        "        texts: list[str]\n",
        "        normalize: bool - å‡ºåŠ›ãƒ™ã‚¯ãƒˆãƒ«ã‚’æ­£è¦åŒ–ã™ã‚‹ã‹ã©ã†ã‹\n",
        "        \"\"\"\n",
        "        if isinstance(texts, str):\n",
        "            texts = [texts]\n",
        "\n",
        "        inputs = self.tokenizer(\n",
        "            texts,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\",\n",
        "        ).to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**inputs)\n",
        "            emb = outputs.last_hidden_state.mean(dim=1)  # (B, H)\n",
        "\n",
        "        proj = self.projector(emb)  # (B, out_dim)\n",
        "\n",
        "        if normalize:\n",
        "            proj = proj / (proj.norm(dim=1, keepdim=True) + 1e-8)\n",
        "\n",
        "        return proj"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "oCmyFnLHsVfc"
      },
      "outputs": [],
      "source": [
        "class ImageEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    MobileNetV3-Smallã§ç”»åƒã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã™ã‚‹ã‚¯ãƒ©ã‚¹\n",
        "    Linear Projectionã§256æ¬¡å…ƒã«å°„å½±å¤‰æ›\n",
        "    \"\"\"\n",
        "    def __init__(self, out_dim: int = EMBED_DIM, device: torch.device = DEVICE):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        self.model = models.mobilenet_v3_small(pretrained=True)\n",
        "        self.model.eval()\n",
        "        self.features = self.model.features.to(device)\n",
        "        self.projector = nn.Linear(576, out_dim).to(device)  # MobileNetV3-Smallã®æœ€çµ‚ç‰¹å¾´é‡æ¬¡å…ƒæ•°ã¯576\n",
        "\n",
        "        self.transform = T.Compose([\n",
        "            T.Resize((224, 224)),\n",
        "            T.ToTensor(),\n",
        "            T.Normalize([0.485, 0.456, 0.406],\n",
        "                        [0.229, 0.224, 0.225]),\n",
        "        ])\n",
        "\n",
        "    def encode(self, images, normalize: bool = True) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        ç”»åƒãƒªã‚¹ãƒˆã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ã¦ãƒ™ã‚¯ãƒˆãƒ«ã‚’è¿”ã™é–¢æ•°\n",
        "\n",
        "        images: list[PIL.Image.Image] ã¾ãŸã¯ å˜ä¸€ã®PIL.Image.Image\n",
        "        normalize: bool - å‡ºåŠ›ãƒ™ã‚¯ãƒˆãƒ«ã‚’æ­£è¦åŒ–ã™ã‚‹ã‹ã©ã†ã‹\n",
        "        \"\"\"\n",
        "        single = False\n",
        "        if not isinstance(images, (list, tuple)):\n",
        "            images = [images]\n",
        "            single = True\n",
        "\n",
        "        tensors = []\n",
        "        for img in images:\n",
        "            pil_img = load_image(img)\n",
        "            tensor = self.transform(pil_img) # (3, H, W)\n",
        "            tensors.append(tensor)\n",
        "        batch = torch.stack(tensors, dim=0).to(self.device)  # (B, 3, H, W)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            feat = self.features(batch)  # (B, 576, 7, 7)\n",
        "            feat = feat.mean(dim=[2, 3])  # (B, 576)\n",
        "\n",
        "        proj = self.projector(feat)  # (B, out_dim)\n",
        "\n",
        "        if normalize:\n",
        "            proj = proj / (proj.norm(dim=1, keepdim=True) + 1e-8)\n",
        "\n",
        "        if single:\n",
        "            return proj # (1, out_dim)\n",
        "        return proj  # (B, out_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "tKkPeBRlsVfc"
      },
      "outputs": [],
      "source": [
        "class MiniCLIP:\n",
        "    \"\"\"\n",
        "    TextEncoder ã¨ ImageEncoder ã‚’å…±æœ‰ 256æ¬¡å…ƒç©ºé–“ã«æƒãˆã€\n",
        "    CLIPé¢¨ã®å¯¾ç…§å­¦ç¿’ã§ projector å±¤ã®ã¿ã‚’å­¦ç¿’ã™ã‚‹ã‚¯ãƒ©ã‚¹ã€‚\n",
        "    \"\"\"\n",
        "    def __init__(self, text_encoder: TextEncoder, image_encoder: ImageEncoder, temperature: float = 0.07):\n",
        "        self.text_encoder = text_encoder\n",
        "        self.image_encoder = image_encoder\n",
        "        self.temperature = temperature\n",
        "\n",
        "        params = list(self.text_encoder.projector.parameters()) + \\\n",
        "                 list(self.image_encoder.projector.parameters())\n",
        "        self.optimizer = AdamW(params, lr=1e-4)\n",
        "\n",
        "\n",
        "    def compute_loss(self, img_vecs: torch.Tensor, txt_vecs: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        CLIPé¢¨ã®å¯¾ç…§å­¦ç¿’ã®æå¤±ã‚’è¨ˆç®—ã™ã‚‹é–¢æ•°\n",
        "        img_vecs: (B, D)\n",
        "        txt_vecs: (B, D)\n",
        "        \"\"\"\n",
        "        # é¡ä¼¼åº¦è¡Œåˆ—[B, B]ã‚’è¨ˆç®—\n",
        "        sim_matrix = torch.matmul(img_vecs, txt_vecs.T)\n",
        "        sim_matrix = sim_matrix / self.temperature\n",
        "\n",
        "        labels = torch.arange(len(img_vecs), device=sim_matrix.device)\n",
        "\n",
        "        loss_img2txt = F.cross_entropy(sim_matrix, labels)\n",
        "        loss_txt2img = F.cross_entropy(sim_matrix.T, labels)\n",
        "\n",
        "        loss = (loss_img2txt + loss_txt2img) / 2.0\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def train(self, pairs, epochs: int = 10, batch_size: int = 32):\n",
        "        \"\"\"\n",
        "        å­¦ç¿’ã‚’å®Ÿè¡Œã™ã‚‹é–¢æ•°\n",
        "        pairs: list of (PIL.Image.Image, str)\n",
        "        \"\"\"\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            texts  = [p[\"text\"]  for p in pairs]\n",
        "            images = [p[\"image\"] for p in pairs]\n",
        "\n",
        "            txt_vecs = self.text_encoder.encode(texts, normalize=True)  # (B, D)\n",
        "            img_vecs = self.image_encoder.encode(images, normalize=True)  # (B, D)\n",
        "\n",
        "            loss = self.compute_loss(img_vecs, txt_vecs)\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "        print(\"Training completed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "HrchQKCWsVfd"
      },
      "outputs": [],
      "source": [
        "class SimpleVectorDB:\n",
        "    \"\"\"\n",
        "    éå¸¸ã«ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã€‚\n",
        "    - items: List[{\"vec\": np.ndarray, \"type\": ..., \"text\": ..., \"image\": ...}]\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.items = []\n",
        "\n",
        "    def add(self, vec: np.ndarray, metadata: dict):\n",
        "        self.items.append({\n",
        "            \"vec\": vec.astype(\"float32\"),\n",
        "            **metadata\n",
        "        })\n",
        "\n",
        "    def build_from_pairs(self, text_encoder: TextEncoder, image_encoder: ImageEncoder, pairs):\n",
        "        \"\"\"\n",
        "        pairs: List[{\"image\": ..., \"text\": ...}]\n",
        "        \"\"\"\n",
        "        self.items = []\n",
        "        for p in pairs:\n",
        "            # text embedding\n",
        "            t_vec = text_encoder.encode(p[\"text\"]).cpu().detach().numpy()[0]\n",
        "            self.add(t_vec, {\n",
        "                \"type\": \"text\",\n",
        "                \"text\": p[\"text\"],\n",
        "                \"image\": p[\"image\"],\n",
        "            })\n",
        "\n",
        "            # image embedding\n",
        "            i_vec = image_encoder.encode(p[\"image\"]).cpu().detach().numpy()[0]\n",
        "            self.add(i_vec, {\n",
        "                \"type\": \"image\",\n",
        "                \"text\": p[\"text\"],\n",
        "                \"image\": p[\"image\"],\n",
        "            })\n",
        "\n",
        "    def search(self, query_vec: np.ndarray, top_k: int = 5, type_filter: str = None):\n",
        "        \"\"\"\n",
        "        query_vec: np.ndarray [D]\n",
        "        type_filter: \"text\" or \"image\" or None\n",
        "        \"\"\"\n",
        "        results = []\n",
        "        for item in self.items:\n",
        "            if type_filter is not None and item[\"type\"] != type_filter:\n",
        "                continue\n",
        "            sim = cosine_sim(query_vec, item[\"vec\"])\n",
        "            results.append((sim, item))\n",
        "\n",
        "        results.sort(key=lambda x: x[0], reverse=True)\n",
        "        return results[:top_k]\n",
        "\n",
        "    def to_jsonable(self):\n",
        "        \"\"\"\n",
        "        JSONä¿å­˜ç”¨ã« numpy é…åˆ—ã‚’ list ã«å¤‰æ›\n",
        "        \"\"\"\n",
        "        json_items = []\n",
        "        for item in self.items:\n",
        "            j = dict(item)\n",
        "            j[\"vec\"] = item[\"vec\"].tolist()\n",
        "            json_items.append(j)\n",
        "        return json_items\n",
        "\n",
        "    @staticmethod\n",
        "    def from_json(data):\n",
        "        db = SimpleVectorDB()\n",
        "        for item in data:\n",
        "            vec = np.array(item[\"vec\"], dtype=\"float32\")\n",
        "            meta = {k: v for k, v in item.items() if k != \"vec\"}\n",
        "            db.add(vec, meta)\n",
        "        return db\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "prATT5pC0OBx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "class ArtBenchDatasetLoader:\n",
        "    \"\"\"\n",
        "    ArtBench-10 ã®ç°¡æ˜“ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã€‚\n",
        "    å„ãƒ•ã‚©ãƒ«ãƒ€åã‚’ãã®ã¾ã¾ãƒ†ã‚­ã‚¹ãƒˆèª¬æ˜ã¨ã—ã¦ä½¿ã†ã€‚\n",
        "    ãƒ‡ãƒ¼ã‚¿ã¯kagglehubã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n",
        "\n",
        "    limit_per_class: 1ã‚¯ãƒ©ã‚¹ã‚ãŸã‚Šã®æœ€å¤§ãƒ‡ãƒ¼ã‚¿æ•°ã€‚\n",
        "    shuffle: ãƒ‡ãƒ¼ã‚¿ã‚’ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã™ã‚‹ã‹ã©ã†ã‹ã€‚\n",
        "    \"\"\"\n",
        "    def __init__(self, root_dir, limit_per_class=200, shuffle=True):\n",
        "        self.root = root_dir\n",
        "        self.csv_path = os.path.join(root_dir, \"ArtBench-10.csv\")\n",
        "        self.limit = limit_per_class\n",
        "        self.shuffle = shuffle\n",
        "\n",
        "        # CSV ã®æ§‹é€ ã«åŸºã¥ãæ­£ã—ã„ mapping\n",
        "        df = pd.read_csv(self.csv_path)\n",
        "        if \"label\" not in df.columns:\n",
        "            raise RuntimeError(f\"'label' åˆ—ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {df.columns}\")\n",
        "\n",
        "        self.label_list = df[\"label\"].tolist()\n",
        "\n",
        "\n",
        "    def _load_batch(self, file_path):\n",
        "        batch = unpickle(file_path)\n",
        "        data = batch[b\"data\"]              # (N, 3072)\n",
        "        labels = batch[b\"labels\"]          # list of ints\n",
        "        data = data.reshape(-1, 3, 32, 32) # CIFAR shape\n",
        "        return data, labels\n",
        "\n",
        "\n",
        "    def _to_pil(self, arr3x32x32):\n",
        "        \"\"\"numpy array (3,32,32) â†’ PIL.Image\"\"\"\n",
        "        # CIFARå½¢å¼ (C,H,W) â†’ (H,W,C)\n",
        "        img = np.transpose(arr3x32x32, (1, 2, 0))\n",
        "        return Image.fromarray(img.astype(np.uint8))\n",
        "\n",
        "\n",
        "    def load(self):\n",
        "        \"\"\"\n",
        "        ç”»åƒ + ãƒ†ã‚­ã‚¹ãƒˆ (style ã®èª¬æ˜) ã®ãƒšã‚¢ã‚’è¿”ã™ã€‚\n",
        "        \"\"\"\n",
        "        batch_files = [\n",
        "            f for f in os.listdir(self.root)\n",
        "            if f.startswith(\"data_batch\") or f == \"test_batch\"\n",
        "        ]\n",
        "        batch_files.sort()\n",
        "\n",
        "        pairs = []\n",
        "\n",
        "        for bf in batch_files:\n",
        "            data, labels = self._load_batch(os.path.join(self.root, bf))\n",
        "\n",
        "            per_class_count = {}\n",
        "\n",
        "            for i, lid in enumerate(labels):\n",
        "\n",
        "                per_class_count.setdefault(lid, 0)\n",
        "                if per_class_count[lid] >= self.limit:\n",
        "                    continue\n",
        "\n",
        "                style_name = self.label_list[i]  # CSVã® 'label' ã¯ cifar_index ã§å¯¾å¿œ\n",
        "\n",
        "                img_pil = self._to_pil(data[i])\n",
        "                text = f\"{style_name} ã®çµµç”»\"\n",
        "\n",
        "                pairs.append({\n",
        "                    \"image\": img_pil,\n",
        "                    \"text\": text\n",
        "                })\n",
        "\n",
        "                per_class_count[lid] += 1\n",
        "\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(pairs)\n",
        "\n",
        "        print(f\"Loaded {len(pairs)} image-text pairs from ArtBench10.\")\n",
        "        return pairs\n"
      ],
      "metadata": {
        "id": "N4fNekJw0Q5M"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7VEZrgrsVfd",
        "outputId": "84c5b7aa-e6b3-4d59-d6a1-6009d1cfe109"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4,\n",
              " {'image': 'https://images.dog.ceo/breeds/chow/n02112137_16777.jpg',\n",
              "  'text': 'ç™½ã„ãƒ•ãƒ¯ãƒ•ãƒ¯ã®çŠ¬'})"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "class DogDemoDatasetLoader:\n",
        "    \"\"\"\n",
        "    ã¨ã‚Šã‚ãˆãš dog.ceo ã®ãƒ‡ãƒ¢ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™ Loaderã€‚\n",
        "    å°†æ¥ã€ArtBenchLoader / MemotionLoader / CustomLoader ã‚’åŒã˜IFã§è¿½åŠ ã§ãã‚‹ã€‚\n",
        "    \"\"\"\n",
        "    def load(self):\n",
        "        pairs = [\n",
        "            {\n",
        "                \"image\": \"https://images.dog.ceo/breeds/chow/n02112137_16777.jpg\",\n",
        "                \"text\": \"ç™½ã„ãƒ•ãƒ¯ãƒ•ãƒ¯ã®çŠ¬\"\n",
        "            },\n",
        "            {\n",
        "                \"image\": \"https://images.dog.ceo/breeds/pitbull/20190710_143021.jpg\",\n",
        "                \"text\": \"é»’ã¨ç™½ã®çŠ¬ãŒç·‘ã®è‰åŸã®ä¸­ã«ã„ã‚‹\"\n",
        "            },\n",
        "            {\n",
        "                \"image\": \"https://images.dog.ceo/breeds/poodle-toy/n02113624_9550.jpg\",\n",
        "                \"text\": \"ãµã‚ãµã‚ã®å­çŠ¬ã®å†™çœŸ\"\n",
        "            },\n",
        "            {\n",
        "                \"image\": \"https://images.dog.ceo/breeds/eskimo/n02109961_8185.jpg\",\n",
        "                \"text\": \"ç™½ã„çŠ¬ãŒçŸ³ç•³ã®ä¸Šã«åº§ã£ã¦ã„ã‚‹\"\n",
        "            }\n",
        "        ]\n",
        "        return pairs\n",
        "\n",
        "loader = DogDemoDatasetLoader()\n",
        "image_text_pairs = loader.load()\n",
        "len(image_text_pairs), image_text_pairs[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6JtaxwPsVfd"
      },
      "source": [
        "# MiniClipä½œã£ã¦å­¦ç¿’"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DownloadDataset\n",
        "!mkdir -p ./artbench\n",
        "!kaggle datasets download alexanderliao/artbench10 -p ./artbench --unzip"
      ],
      "metadata": {
        "id": "CRzscPEFFw2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "id": "YJxCsh5HsVfd",
        "outputId": "e5300899-2d54-4d26-9fbe-e605bab9dc05"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ArtBench-10.csv ã®åˆ—æ§‹é€ ãŒæƒ³å®šã¨é•ã„ã¾ã™",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-247662634.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mArtBenchDatasetLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./artbench\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit_per_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# (image, text) ãƒšã‚¢ã‚’å–å¾—\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpairs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4281384702.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root_dir, limit_per_class, shuffle)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# cifar_index = CIFAR ã®ãƒ©ãƒ™ãƒ«\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"cifar_index\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"style\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ArtBench-10.csv ã®åˆ—æ§‹é€ ãŒæƒ³å®šã¨é•ã„ã¾ã™\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_to_style\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"cifar_index\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"style\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: ArtBench-10.csv ã®åˆ—æ§‹é€ ãŒæƒ³å®šã¨é•ã„ã¾ã™"
          ]
        }
      ],
      "source": [
        "loader = ArtBenchDatasetLoader(\"./artbench\", limit_per_class=200)\n",
        "# (image, text) ãƒšã‚¢ã‚’å–å¾—\n",
        "pairs = loader.load()\n",
        "print(len(pairs))\n",
        "\n",
        "# MiniCLIP ã«æŠ•å…¥\n",
        "# mini_clip.train(pairs, epochs=10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"/content/artbench/ArtBench-10.csv\")  # â†ãƒ‘ã‚¹ã¯åˆã‚ã›ã¦\n",
        "print(df.head())\n",
        "print(df.columns)\n"
      ],
      "metadata": {
        "id": "PnNidz9TPaO2",
        "outputId": "5f408dc2-ddff-4956-acd0-aba0d2247d63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                            name              artist  \\\n",
            "0      frank-omeara_towards-night-and-winter.jpg        frank-omeara   \n",
            "1                 goldstein-grigoriy_morning.jpg  goldstein-grigoriy   \n",
            "2                 georges-lemmen_man-reading.jpg      georges-lemmen   \n",
            "3       theodor-aman_port-of-constantza-1882.jpg        theodor-aman   \n",
            "4  niccolo-cannicci_il-passo-della-futa-1914.jpg    niccolo-cannicci   \n",
            "\n",
            "                                                 url  is_public_domain  \\\n",
            "0  https://uploads5.wikiart.org/00316/images/fran...              True   \n",
            "1  https://uploads5.wikiart.org/images/grigoriy-g...              True   \n",
            "2  https://uploads6.wikiart.org/images/georges-le...              True   \n",
            "3  https://uploads6.wikiart.org/images/theodor-am...              True   \n",
            "4  https://uploads3.wikiart.org/images/niccolo-ca...              True   \n",
            "\n",
            "   length  width          label  split  cifar_index  \n",
            "0     800    657  impressionism  train        43186  \n",
            "1     521    499  impressionism  train        41151  \n",
            "2     800    612  impressionism  train         9754  \n",
            "3     560    336  impressionism  train        44244  \n",
            "4    2400   2322  impressionism  train        46885  \n",
            "Index(['name', 'artist', 'url', 'is_public_domain', 'length', 'width', 'label',\n",
            "       'split', 'cifar_index'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99PuI5zzsVfd"
      },
      "source": [
        "# VectorDBæ§‹ç¯‰ï¼†æ¤œç´¢ï¼†å¯è¦–åŒ–"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKDt7CYbsVfd"
      },
      "outputs": [],
      "source": [
        "# VectorDBæ§‹ç¯‰\n",
        "vecdb = SimpleVectorDB()\n",
        "vecdb.build_from_pairs(text_encoder, image_encoder, image_text_pairs)\n",
        "print(f\"VectorDB size: {len(vecdb.items)} items\")\n",
        "\n",
        "# ãƒ†ã‚­ã‚¹ãƒˆã§ã‚¯ã‚¨ãƒª\n",
        "query_text = \"ç™½ã„çŠ¬\"\n",
        "query_vec = text_encoder.encode(query_text).cpu().detach().numpy()[0]\n",
        "\n",
        "results = vecdb.search(query_vec, top_k=5)\n",
        "for sim, item in results:\n",
        "    print(f\"sim={sim:.3f} | type={item['type']} | text={item['text']} | image={item['image']}\")\n",
        "    img = load_image(item[\"image\"])\n",
        "    img_small = img.resize((150, 100))\n",
        "    display(img_small)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40bEOaRtsVfe"
      },
      "source": [
        "# UMAP å¯è¦–åŒ–"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WA-Z8pMssVfe"
      },
      "outputs": [],
      "source": [
        "embs = np.stack([it[\"vec\"] for it in vecdb.items], axis=0)\n",
        "labels = [it[\"type\"] for it in vecdb.items]\n",
        "texts = [it[\"text\"] for it in vecdb.items]\n",
        "\n",
        "# plot_umap_matplotlib(embs, labels, texts, title=\"Mini-CLIP: Dog Demo (Matplotlib)\")\n",
        "plot_umap_plotly(embs, labels, texts, title=\"Mini-CLIP: Dog Demo (Plotly)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hBAR4rwsVfe"
      },
      "source": [
        "# Export image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w8YgHYhfsVfe"
      },
      "outputs": [],
      "source": [
        "os.makedirs(\"artifacts\", exist_ok=True)\n",
        "\n",
        "# projector ä¿å­˜\n",
        "torch.save(text_encoder.projector.state_dict(), \"artifacts/text_projector.pt\")\n",
        "torch.save(image_encoder.projector.state_dict(), \"artifacts/image_projector.pt\")\n",
        "\n",
        "# VectorDB ä¿å­˜\n",
        "with open(\"artifacts/vector_db.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(vecdb.to_jsonable(), f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"Saved to ./artifacts/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AARYzRXm1AT8"
      },
      "source": [
        "#### æ—¥æœ¬èªè¡¨ç¤ºå¯¾å¿œ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pjxD2GP7sVfe"
      },
      "outputs": [],
      "source": [
        "# # æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã€‚\n",
        "# !apt-get -y install fonts-ipafont-gothic\n",
        "# import matplotlib.pyplot as plt\n",
        "# plt.rcParams['font.family'] = 'IPAGothic'\n",
        "\n",
        "# import shutil\n",
        "# import os\n",
        "# # ãƒ•ã‚©ãƒ³ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’å‰Šé™¤\n",
        "# font_cache_path = os.path.expanduser(\"~/.cache/matplotlib\")\n",
        "# if os.path.exists(font_cache_path):\n",
        "#     shutil.rmtree(font_cache_path)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}