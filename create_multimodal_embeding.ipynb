{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/OlyKoek/MyStudy-AI-app/blob/colab/create_multimodal_embeding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SLj01Un8xV5g"
   },
   "source": [
    "# マルチモーダルなエンベディングモデルの作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AARYzRXm1AT8"
   },
   "source": [
    "#### 日本語表示対応"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uG7N2QVN0_nC",
    "outputId": "5f4051fd-cfce-484e-c60d-4dd0a3a229ee"
   },
   "outputs": [],
   "source": [
    "# 日本語フォントをダウンロードする。\n",
    "!apt-get -y install fonts-ipafont-gothic\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.family'] = 'IPAGothic'\n",
    "\n",
    "import shutil\n",
    "import os\n",
    "# フォントキャッシュを削除\n",
    "font_cache_path = os.path.expanduser(\"~/.cache/matplotlib\")\n",
    "if os.path.exists(font_cache_path):\n",
    "    shutil.rmtree(font_cache_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uq4xXAPix-Tz"
   },
   "source": [
    "### Pytorchのパッケージと必要なライブラリをインストール\n",
    "- torch   \n",
    "- transformers from hugging face\n",
    "- umap-learn : 可視化で使ってみたい。t-SNEよりも良いらしい\n",
    "- matplotlib : ベクトル空間の可視化に使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gDh344AzxRVD"
   },
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio --quiet\n",
    "!pip install transformers umap-learn matplotlib pandas scikit-learn --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Yq9qGixyPA9"
   },
   "source": [
    "# 概要\n",
    "やりたいことは、マルチモーダルな入力をベクトル空間に投影すること。\n",
    "今回はできるだけ簡単に構築したいので、下記の構成。\n",
    "\n",
    "- テキスト：TinyBERT系   \n",
    "(最初all-MiniLM-L6-v2を使ったけど、日本語使えないことに気が付いたので一旦sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2に変更。東北大学が作った日本語専用のbert-base-japanese-v2もよさげだけど。一旦これで行く。)\n",
    "- 画像：CLIP\n",
    "- 音：MCFF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tg187dqNDWT9"
   },
   "source": [
    "## Step1 テキストのEmbedding\n",
    "\n",
    "ここでは、大規模コーパスで学習済のTinyBERTを**特徴量抽出器**として扱い、\n",
    "その出力をLiner層(Projection Head)を使って、マルチモーダル用の次元圧縮と\n",
    "好きな意味空間になるように調整を行う。ざっくり言うと、プロジェクションヘッドを使った転移学習。\n",
    "\n",
    "step1: TinyBERTにテキストを入力してエンコード   \n",
    "step2: エンコード結果をLiner層で次元圧縮しつつ、任意の意味空間になるような学習をさせる    \n",
    "Step3: 好きなテキストをTinyBERT+Linerに入れると、Step2の学習に従ったベクトル意味空間にプロットされる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qC7oQsorqbAJ"
   },
   "source": [
    "### ■ tinyBERTをダウンロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 288,
     "referenced_widgets": [
      "735afc0009b3473b8a15833f162e2365",
      "0cb63040c868478f84aaeddaf073ef45",
      "5870c38d7b814d34af8582f8424434f3",
      "164067902487480f86b27e40097d27d5",
      "55be28918913429c918c7889667d6158",
      "3d22e8fcbce54e0db76d166e6e00fb6e",
      "58457e11dccf4ecc9516ed08294feb59",
      "abb04b62534445b6ad7da348d1ee459e",
      "bd51749e25ac4206a2aa1244876b7288",
      "a614cfe6c0904ee19530ee9f2e9c8844",
      "bb7b33088c8341e58f5c150d9af6c775",
      "e11d7d2c86194710ab8958b04ff4453c",
      "739ddbb0dee84082a8a87224107a18ac",
      "4aadf63a0ad149348e95dead95fb97d4",
      "a07cdb993f284afe81fa7dde21a9c9a8",
      "ada8d519b4b2456c9fab2166b22e0f05",
      "afccc055006b47e783c0478c6ee258e6",
      "8343e2b31a424937b96a95f7ec98e547",
      "4866ef42557d4b6cabeba9cffeddbe93",
      "a23e402cf04e454fafe265379648ada6",
      "d676341cac484c49a2a29fc045ed339d",
      "b9e79e6430d94a1eaed9f9aa42090a82",
      "0478806893894155b80970ee7e163545",
      "51013607d7a143b29e6b6d864947d1d5",
      "3e2dfe4654ae4e3f9d54c679fdda131b",
      "e5c946bf4c054c94aba556a386b77f9b",
      "2beb36ab3feb4442a49eee408c5f91af",
      "3d53c8f1744045658a467bd1a8850a50",
      "0eb7cfc47f494184843217d0b4fb4db7",
      "8009cbe9dc11417b8110648241001235",
      "f2349e4b517e451385e223887ac9a342",
      "9791153be71944ff86f6fee7aacb3a57",
      "bc23c8f24e134478b019ba7b5ac665ee",
      "9a15aeab5f9c417c9069561042712c2f",
      "e9c4e539f07346f6bcbab13a859b1527",
      "27a7424e62b94d8399cd0d40bd85f58d",
      "3f4bccce5f5349e0b68eea58f4796dc2",
      "984876e7bc9d4381b2fc3bd7257af480",
      "42a924060e6f44f08209e0d3dfdd583d",
      "1177bcf2516844fd891a76f20169bce4",
      "18edf7d7d93445a19750f30fe19a2bcc",
      "be27a783eb1b4dae92af9d27f342bb4c",
      "f92f2ecc6b9249d4a9fadd50249f83b0",
      "f691d6b9b71e422ca2eb4cb82c146ec6",
      "3a9b89ea39c94caab5f65147fbf7afc6",
      "adadbb4bb22d4d7db1d5042feb7ccffd",
      "528f59f27d144721a2c648d0ac16596e",
      "09fec7d15dc64bb7b3b775d16ad803f8",
      "42052c2b371542ceaa491fbd21d528c9",
      "f6399bddafae468ab0b7e691ea82888b",
      "f1d07480031846509cfa605d8653eea5",
      "6425ee3b071f46fca4e1ebe7766cf322",
      "3acd018e979842678bf314fd81fc0cf8",
      "102801d484cf4b1babc2141fdaeb4f49",
      "fd82c8b372294c2e9dae46cd835fcf15"
     ]
    },
    "id": "UAlang7eykUg",
    "outputId": "60064b08-b7dd-4cc9-fc20-8436b9ae274b"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "\n",
    "# read TinyBERT model\n",
    "MODEL_NAME = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8jPdVfYwrMrS"
   },
   "source": [
    "(動作確認用)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KViGhsxsqLqn",
    "outputId": "862cb9b1-bb4a-4f14-fe2f-78812fed6947"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "# test sample text\n",
    "texts = [\"今日めっちゃ良かった楽しい\",\"マジつまんね。つらい\",\"今日は普通かな特に何もないし\"]\n",
    "\n",
    "# tokenize\n",
    "inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "# test ==========================\n",
    "# view to tokenizerd text\n",
    "print(\"-\"*50)\n",
    "text = texts[0]\n",
    "tokens = tokenizer.tokenize(text)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(f\"Tokenizeの出力確認\\n\\n 入力文章：{text} \\nトークン文字：{tokens} \\nトークンID: {ids}\")\n",
    "print(\"-\"*50)\n",
    "# ==============================\n",
    "\n",
    "\n",
    "# input tokenized data to TinyBERT model\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# convert token to vector\n",
    "embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "print(\"\\n文数 =\",embeddings.shape[0],\" ベクトル次元 =\", embeddings.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5BzsW1c_sYet"
   },
   "source": [
    "### ■ Linearヘッドを追加して256次元に射影\n",
    "\n",
    " 射影：今回使ったTinyBERTのディフォルトの埋め込みベクトル次元空間を、別のベクトル次元目的空間に変換するための、線形変換を適応\n",
    "\n",
    " 今回はテキスト以外も同じ特徴空間にプロットするので、共有可能なベクトル次元空間にする必要がある。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QW62QQsqsfU2",
    "outputId": "71dc4da1-a2a3-4acd-81ad-f729fcd9bb08"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "projector = nn.Linear(embeddings.shape[1], 256)\n",
    "\n",
    "#射影\n",
    "projected = projector(embeddings)\n",
    "print(projected.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hn4AR5uWDv0s"
   },
   "source": [
    "#### ■ 対象学習　ベクトル同士の意味付け\n",
    "\n",
    "似た意味の単語を近く人、反対の意味の単語を遠くに、という形でベクトル空間上のプロットを最適化して、自然にクラスタ化されるようにする。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "id": "yxXN18M-6TD0",
    "outputId": "2be5f1cf-370d-46bf-f4db-91a476a7b7ce"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    \"anchor\": [\"嬉しい\", \"悲しい\", \"疲れた\", \"ワクワクする\", \"寂しい\", \"楽しい\"],\n",
    "    \"positive\": [\"楽しい\", \"つらい\", \"だるい\", \"楽しみ\", \"悲しい\", \"嬉しい\"],\n",
    "    \"negative\": [\"怒っている\", \"嬉しい\", \"ワクワクする\", \"眠い\", \"明るい\", \"悲しい\"]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vlSplsB0qIJJ",
    "outputId": "4963fcd9-80d9-41e1-f74f-049e69eff868"
   },
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "def encode_texts(text_list):\n",
    "  inputs = tokenizer(text_list, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "  with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "# 各文をエンコード\n",
    "anchor_emb = encode_texts(df[\"anchor\"].tolist())\n",
    "pos_emb = encode_texts(df[\"positive\"].tolist())\n",
    "neg_emb = encode_texts(df[\"negative\"].tolist())\n",
    "\n",
    "# 正例：anchorとposを近づける\n",
    "pos_sim = F.cosine_similarity(anchor_emb, pos_emb)\n",
    "# 負例：anchorとnegを離す\n",
    "neg_sim = F.cosine_similarity(anchor_emb, neg_emb)\n",
    "\n",
    "# InfoNCE風のloss（シンプル版）\n",
    "loss = -torch.log(torch.exp(pos_sim) / (torch.exp(pos_sim) + torch.exp(neg_sim)))\n",
    "loss = loss.mean()\n",
    "print(f\"Contrastive Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_HTaJxxOIrcf"
   },
   "source": [
    "  ■ 調整したベクトル間の関係でLinerを学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8HCeRIiUrDWE",
    "outputId": "3803e0ae-f675-4645-8f3d-1082ae1f9ed3"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(projector.parameters(), lr=1e-4)\n",
    "\n",
    "for epoch in range(10):\n",
    "    optimizer.zero_grad()\n",
    "    anchor_emb = encode_texts(df[\"anchor\"].tolist())\n",
    "    pos_emb = encode_texts(df[\"positive\"].tolist())\n",
    "    neg_emb = encode_texts(df[\"negative\"].tolist())\n",
    "\n",
    "    pos_sim = F.cosine_similarity(projector(anchor_emb), projector(pos_emb))\n",
    "    neg_sim = F.cosine_similarity(projector(anchor_emb), projector(neg_emb))\n",
    "    loss = -torch.log(torch.exp(pos_sim) / (torch.exp(pos_sim) + torch.exp(neg_sim))).mean()\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {epoch+1} | Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "5lyUXE5vs8Gk",
    "outputId": "fd6f233f-8e68-4a7b-91ac-d321fbbb9ade"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "all_texts = df[\"anchor\"].tolist() + df[\"positive\"].tolist() + df[\"negative\"].tolist()\n",
    "embeddings = encode_texts(all_texts)\n",
    "projected = projector(embeddings).detach().numpy()\n",
    "\n",
    "import umap, matplotlib.pyplot as plt\n",
    "reducer = umap.UMAP(metric=\"cosine\", n_neighbors=5, min_dist=0.3)\n",
    "umap_emb = reducer.fit_transform(projected)\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(umap_emb[:,0], umap_emb[:,1], c=\"orange\", alpha=0.7)\n",
    "for i, t in enumerate(all_texts):\n",
    "    plt.text(umap_emb[i,0], umap_emb[i,1], t, fontsize=9)\n",
    "plt.title(\"Emotion Space after Contrastive Learning\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lpmTZTg7Zf18"
   },
   "source": [
    "## Step2 画像のEmbedding\n",
    "\n",
    "ここでは、大規模コーパスで学習済のTinyBERTを**特徴量抽出器**として扱い、\n",
    "その出力をLiner層(Projection Head)を使って、マルチモーダル用の次元圧縮と\n",
    "好きな意味空間になるように調整を行う。ざっくり言うと、プロジェクションヘッドを使った転移学習。\n",
    "\n",
    "step1: TinyBERTにテキストを入力してエンコード   \n",
    "step2: エンコード結果をLiner層で次元圧縮しつつ、任意の意味空間になるような学習をさせる    \n",
    "Step3: 好きなテキストをTinyBERT+Linerに入れると、Step2の学習に従ったベクトル意味空間にプロットされる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UzGuoxDTZrIp",
    "outputId": "df751d78-751f-4233-9608-833f53f677c9"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "# MobileNetV3 Small\n",
    "image_model = models.mobilenet_v3_small(pretrained=True)\n",
    "image_model.eval()  # 使用するエンコーダは凍結\n",
    "\n",
    "# 最終層直前の512次元ベクトルを得るためのhook\n",
    "# MobileNetV3の特徴抽出部だけを使う\n",
    "feature_extractor = image_model.features\n",
    "\n",
    "# 画像前処理\n",
    "image_transform = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.485, 0.456, 0.406],\n",
    "                [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# 画像のエンコード用関数を作成\n",
    "def encode_image(url):\n",
    "    response = requests.get(url)\n",
    "    img = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "    tensor = image_transform(img).unsqueeze(0)  # [1,3,224,224]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        feat = feature_extractor(tensor)  # [1, 576, 7,7]\n",
    "        feat = torch.mean(feat, dim=[2,3])  # 平均プーリング → [1, 576]\n",
    "\n",
    "    return feat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gIDTtBRJJKou"
   },
   "source": [
    "（動作確認用）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VNHasm2QZur9",
    "outputId": "43b89b27-148f-48b2-842f-0c39917749df"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "# テスト画像（犬）\n",
    "test_url = \"https://images.dog.ceo/breeds/chow/n02112137_16777.jpg\"\n",
    "img_emb = encode_image(test_url)\n",
    "print(img_emb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qeB4Hb0XJNmD"
   },
   "source": [
    "# Step3 各種エンコーダーを整理して、それぞれの入力の関連性を使った学習の準備。\n",
    "同じ特徴空間を指していることを確認。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TP-BUFctpPew"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# MobileNetV3-Small の最終特徴量は 576 次元\n",
    "image_projector = nn.Linear(576, 256)\n",
    "# TinyBERT 用 projector（すでにあるはず）\n",
    "text_projector = projector  # 前フェーズで作った 384 → 256 Linear\n",
    "\n",
    "\n",
    "# 学習用のエンコーダー用関数を作成\n",
    "def encode_text_for_train(text):\n",
    "    emb = encode_texts([text])           # TinyBERT（固定）\n",
    "    proj = text_projector(emb)           # projector（学習対象）\n",
    "    proj = proj / (proj.norm(dim=-1, keepdim=True) + 1e-8)\n",
    "    return proj\n",
    "\n",
    "def encode_image_for_train(url):\n",
    "    feat = encode_image(url)             # MobileNet（固定）\n",
    "    proj = image_projector(feat)         # projector（学習対象）\n",
    "    proj = proj / (proj.norm(dim=-1, keepdim=True) + 1e-8)\n",
    "    return proj\n",
    "\n",
    "# 推論用のエンコーダー用関数を作成\n",
    "def encode_text_256(text):\n",
    "    with torch.no_grad():\n",
    "        emb = encode_texts([text])      # TinyBERT → [1,384]\n",
    "        proj = text_projector(emb)      # projector → [1,256]\n",
    "        proj = proj / (proj.norm(dim=-1, keepdim=True) + 1e-8)\n",
    "    return proj\n",
    "\n",
    "def encode_image_256(url):\n",
    "    with torch.no_grad():\n",
    "        feat = encode_image(url)        # MobileNet → [1,576]\n",
    "        proj = image_projector(feat)    # projector → [1,256]\n",
    "        proj = proj / (proj.norm(dim=-1, keepdim=True) + 1e-8)\n",
    "    return proj\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lq-jiIITKcm6"
   },
   "source": [
    "■ データセット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w-X3YYHKpTj5",
    "outputId": "287ef0fb-4d07-4b1f-ffa6-8178c740eff4"
   },
   "outputs": [],
   "source": [
    "image_text_pairs = [\n",
    "    {\n",
    "        \"url\": \"https://images.dog.ceo/breeds/chow/n02112137_16777.jpg\",\n",
    "        \"text\": \"白いフワフワの犬\"\n",
    "    },\n",
    "    {\n",
    "        \"url\": \"https://images.dog.ceo/breeds/pitbull/20190710_143021.jpg\",\n",
    "        \"text\": \"黒と白の犬が緑の草原の中にいる\"\n",
    "    },\n",
    "    {\n",
    "        \"url\": \"https://images.dog.ceo/breeds/poodle-toy/n02113624_9550.jpg\",\n",
    "        \"text\": \"ふわふわの子犬の写真\"\n",
    "    },\n",
    "    {\n",
    "        \"url\": \"https://images.dog.ceo/breeds/eskimo/n02109961_8185.jpg\",\n",
    "        \"text\": \"白い犬が石畳の上に座っている\"\n",
    "    }\n",
    "]\n",
    "\n",
    "img_vec = encode_image_256(image_text_pairs[0][\"url\"])\n",
    "txt_vec = encode_text_256(image_text_pairs[0][\"text\"])\n",
    "print(img_vec.shape, txt_vec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9zTwhaiZP-rJ"
   },
   "source": [
    "■　対照学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "naxlZiuV2siH",
    "outputId": "9299c6e2-a198-456b-a5a0-caded85f820e"
   },
   "outputs": [],
   "source": [
    "# clipを参考に事前に作成した、projection head層だけを学習する。\n",
    "optimizer = torch.optim.AdamW(\n",
    "    list(image_projector.parameters()) +\n",
    "    list(text_projector.parameters()),\n",
    "    lr=1e-4\n",
    ")\n",
    "\n",
    "\n",
    "# Enbeddingされた画像ベクトルとテキストベクトルの内積を全部取得\n",
    "def contrastive_loss(img_vecs, txt_vecs):\n",
    "    # cosine similarity matrix\n",
    "    sim_matrix = torch.matmul(\n",
    "        img_vecs,\n",
    "        txt_vecs.T\n",
    "    )  # [batch, batch]\n",
    "\n",
    "    # 温度パラメータ（CLIP論文と同じ）\n",
    "    temperature = 0.07\n",
    "    sim_matrix = sim_matrix / temperature\n",
    "\n",
    "    # 正解 = 対角線ペア\n",
    "    labels = torch.arange(len(sim_matrix)).long()\n",
    "\n",
    "    # 学習データ通り画像とテキストのペアで学習されるようにする。\n",
    "    loss_i = torch.nn.CrossEntropyLoss()(sim_matrix, labels)\n",
    "    loss_t = torch.nn.CrossEntropyLoss()(sim_matrix.T, labels)\n",
    "    return (loss_i + loss_t) / 2\n",
    "\n",
    "\n",
    "# 学習ループ\n",
    "for epoch in range(20):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    img_vecs = []\n",
    "    txt_vecs = []\n",
    "\n",
    "    for pair in image_text_pairs:\n",
    "        img_vecs.append(encode_text_for_train(pair[\"text\"]))\n",
    "        txt_vecs.append(encode_image_for_train(pair[\"url\"]))\n",
    "\n",
    "    img_vecs = torch.cat(img_vecs, dim=0)\n",
    "    txt_vecs = torch.cat(txt_vecs, dim=0)\n",
    "\n",
    "    loss = contrastive_loss(img_vecs, txt_vecs)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d-9HN73wKsEv"
   },
   "source": [
    "(動作確認用)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H-Z1-Ins2wVS",
    "outputId": "bd7dbecc-60a9-4a84-84a6-4c0ad0af68cf"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "query = \"ふわふわの犬\"\n",
    "q_vec = encode_text_256(query)\n",
    "\n",
    "sims = torch.nn.functional.cosine_similarity(\n",
    "    q_vec, img_vecs\n",
    ")\n",
    "print(sims)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "8eQg4fLw21lI",
    "outputId": "4f25d92b-cb5b-419e-8797-a465b6fb8393"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "import numpy as np\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "all_vecs = []\n",
    "labels = []\n",
    "\n",
    "# text\n",
    "for pair in image_text_pairs:\n",
    "    all_vecs.append(encode_text_256(pair[\"text\"]).detach().numpy())\n",
    "    labels.append(\"txt:\" + pair[\"text\"])\n",
    "\n",
    "# image\n",
    "for pair in image_text_pairs:\n",
    "    all_vecs.append(encode_image_256(pair[\"url\"]).detach().numpy())\n",
    "    labels.append(\"img\")\n",
    "\n",
    "all_vecs = np.vstack(all_vecs)\n",
    "\n",
    "reducer = umap.UMAP(metric=\"cosine\")\n",
    "umap_vecs = reducer.fit_transform(all_vecs)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "for i, lab in enumerate(labels):\n",
    "    plt.scatter(umap_vecs[i,0], umap_vecs[i,1])\n",
    "    plt.text(umap_vecs[i,0], umap_vecs[i,1], lab[:10], fontsize=8)\n",
    "\n",
    "plt.title(\"Mini-CLIP: Image × Text Shared Space\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NFl9HOGC1ilV"
   },
   "source": [
    "## ベクトルDBを作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uHhz4be6Xl6D",
    "outputId": "c08f3238-b6de-4310-c1ab-f96578fe86ae"
   },
   "outputs": [],
   "source": [
    "vector_db = []  # ここにすべての埋め込みを保存\n",
    "\n",
    "def build_vector_db(pairs):\n",
    "    for pair in pairs:\n",
    "        # テキスト埋め込み\n",
    "        txt_vec = encode_text_256(pair[\"text\"]).cpu().detach().numpy()\n",
    "        vector_db.append({\n",
    "            \"type\": \"text\",\n",
    "            \"text\": pair[\"text\"],\n",
    "            \"url\": pair[\"url\"],   # 画像と紐付いてる場合\n",
    "            \"vec\": txt_vec\n",
    "        })\n",
    "\n",
    "        # 画像埋め込み\n",
    "        img_vec = encode_image_256(pair[\"url\"]).cpu().detach().numpy()\n",
    "        vector_db.append({\n",
    "            \"type\": \"image\",\n",
    "            \"text\": pair[\"text\"],\n",
    "            \"url\": pair[\"url\"],\n",
    "            \"vec\": img_vec\n",
    "        })\n",
    "\n",
    "build_vector_db(image_text_pairs)\n",
    "len(vector_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "roZA1Bhl1fQF"
   },
   "source": [
    "## Vector DBを使用するための関数を作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-1fIrYNw1eza"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import requests\n",
    "from IPython.display import display\n",
    "\n",
    "def cosine_sim(a, b):\n",
    "    # Ensure inputs are 1D arrays for dot product and norm calculation\n",
    "    a_flat = a.flatten()\n",
    "    b_flat = b.flatten()\n",
    "    return np.dot(a_flat, b_flat) / (np.linalg.norm(a_flat) * np.linalg.norm(b_flat) + 1e-8)\n",
    "\n",
    "def search_db(query_vec, top_k=5, mode=None):\n",
    "    results = []\n",
    "    for item in vector_db:\n",
    "\n",
    "        if mode is not None:\n",
    "            if item[\"type\"] != mode:  # \"image\" or \"text\"\n",
    "                continue\n",
    "\n",
    "        sim = cosine_sim(query_vec, item[\"vec\"])\n",
    "        results.append((sim, item))\n",
    "\n",
    "    results.sort(reverse=True, key=lambda x: x[0])\n",
    "    return results[:top_k]\n",
    "\n",
    "def pretty_print_results(results):\n",
    "    for sim, item in results:\n",
    "        img = Image.open(requests.get(item[\"url\"], stream=True).raw)\n",
    "        new_size = (150, 100)\n",
    "        img = img.resize(new_size, Image.Resampling.LANCZOS)\n",
    "        display(img)\n",
    "        print(f\"sim={sim:.3f} | type={item['type']} | text={item['text']} | url={item['url']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u3MXgPTAZbps"
   },
   "source": [
    "■　テスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 609
    },
    "id": "5RFZtQBk1jaQ",
    "outputId": "3bd9c417-debf-4ac7-e8bd-cc3b89a75ae7"
   },
   "outputs": [],
   "source": [
    "query = \"白い犬\"\n",
    "q_vec = encode_text_256(query).cpu().detach().numpy()\n",
    "\n",
    "results = search_db(q_vec, top_k=5)\n",
    "pretty_print_results(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bC6qsT8fnmAT"
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import umap\n",
    "\n",
    "embs = np.array([item[\"vec\"].flatten() for item in vector_db])\n",
    "labels = [item[\"type\"] for item in vector_db]\n",
    "texts = [item[\"text\"] for item in vector_db]\n",
    "urls = [item[\"url\"] for item in vector_db]\n",
    "\n",
    "reducer = umap.UMAP(n_neighbors=10, min_dist=0.1, metric=\"cosine\")\n",
    "coords = reducer.fit_transform(embs)  # shape (N, 2)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"x\": coords[:,0],\n",
    "    \"y\": coords[:,1],\n",
    "    \"type\": labels,\n",
    "    \"text\": texts,\n",
    "    \"url\": urls,\n",
    "})\n",
    "\n",
    "fig = px.scatter(\n",
    "    df,\n",
    "    x=\"x\", y=\"y\",\n",
    "    color=\"type\",          # text / image を色分け\n",
    "    hover_data=[\"text\", \"url\", \"type\"],\n",
    "    title=\"UMAP Visualization (Interactive)\",\n",
    "    width=800, height=600\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4441_hAuX_3h"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM2V5BSBDpravpSMPcLIBN5",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
